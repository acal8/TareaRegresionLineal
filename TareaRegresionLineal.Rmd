---
title: "Tarea: Modelos de regresión lineal simple y múltiple"
subtitle: "Aprendizaje Máquina (I). Máster en Ciencia de Datos - UV"
author: "Adrián Carrasco Alcalá y Clara Montalvá Barcenilla"
date: "Curso 2025-2026"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---

En la librería `MASS` puedes encontrar un famoso banco de datos llamado `Boston` que contiene información sobre 506 barrios de Boston, Massachusetts, en 1970. La base de datos contiene 14 variables relativas a 506 barrios. Para saber qué información está contenida en las variables puedes escribir `?Boston` (después de haber cargado la librería `MASS`).

En esta tarea trabajaremos con el conjunto de datos “boston.xlsx” que encontraréis en el aula virtual (con 199 datos y 11 variables).

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(lmtest)
library(readxl)
library(leaps) 
boston <- read_excel("boston.xlsx")
```

---

<u>Ejercicio 1:</u> Considera la variable respuesta `crim` relacionándola con la variable `X` con la que tenga mayor relación lineal.

```{r}
lm_crim_global <- lm(crim ~. , data=boston)
summary(lm_crim_global)
```

Observamos que la variable más significativa (la de p-valor más pequeño) es `lstat`, por lo que deducimos que es esta la variable con la que `crim` tiene mayor relación lineal. 

1. Evalúa el efecto de `X` sobre `crim`, gráficamente y numéricamente. Es decir, indica como es la relación (fuerza y tipo).

```{r}
lm_crim_lstat <- lm(crim ~ lstat, data=boston)
summary(lm_crim_lstat)
plot(boston$lstat, boston$crim, col='BLUE', xlab = 'lstat', ylab = 'crim')
```

Cuando `lstat` aumenta en una unidad, `crim` aumenta en `r lm_crim_lstat$coefficients[2]`. Vemos que pese a que la variable sea muy significativa, en el gráfico de dispersión se observa una relación de fuerza baja o moderada, ya que los puntos no se agrupan alrededor de ninguna recta y se identifica un aumento en la dispersión de `crim` a medida que `lstat` incrementa. Este efecto es un posible indicador de heterocedasticidad, ya que muestra que la varianza no es constante.

2. Obtén la recta de mínimos cuadrados. Interpreta los resultados obtenidos (coeficientes, significatividad, $R^2$, contraste del modelo, etc...).

```{r}
summary(lm_crim_lstat)
plot(boston$lstat, boston$crim, col='BLUE', main = "Diagrama de dispersión", xlab = 'lstat', ylab = 'crim')
abline(coef=coef(lm_crim_lstat), lwd = 2, col='RED')
```

Obtenemos la siguiente recta de mínimos cuadrados: $$\hat{\text{crim}} = -2.623 + 0.428 \, \, \text{lstat},$$

siendo $\beta_0 =$ `r lm_crim_lstat$coefficients[1]` el intercepto y $\beta_1 =$ `r lm_crim_lstat$coefficients[2]` la pendiente de la recta.

El p-valor del estadístico F es prácticamente $0$, por lo que rechazamos la hipótesis nula de que ninguna variable es significativa para explicar el modelo. Ahora bien, pese a que la variable `lstat` es muy significativa (al 0%) y la recta pasa por el "centro" de los datos, los puntos están muy dispersos alrededor de ella. Esto visualmente respalda el $R^2$ ajustado de $0.4055$ (menor que el del modelo global), confirmando que aún queda más de la mitad de la varianza del modelo por explicar.

Vemos también que la recta tiende a predecir valores negativos de `crim` para valores bajos de `lstat`, lo cual no tiene sentido si tenemos en cuenta que la variable `crim` mide la tasa de criminalidad en la ciudad. Tenemos una subestimación de `crim` para valores bajos y altos de `lstat` y una sobreestimación para valores medios. Además, la dispersión no uniforme de los datos y la concentración de puntos cerca de cero sugiere transformación logarítmica para la relación.

3. Dibuja el diagrama de dispersión, la recta de regresión y las bandas de confianza al 90%.

```{r}
# Obtención de las bandas de estimación
min <- range(boston$lstat)[1]; max <- range(boston$lstat)[2]
nuevos <- data.frame(list(lstat = seq(min,max,length=100)))
bandas_est <- predict(lm_crim_lstat, newdata = nuevos, interval = "confidence", level = 0.90)

# Representación gráfica 
plot(boston$lstat, boston$crim, col='BLUE', main = 'Diagrama de dispersión', xlab = 'lstat', ylab = 'crim')
abline(coef=coef(lm_crim_lstat), col='RED')
lines(nuevos$lstat, bandas_est[,2],col='BLACK')
lines(nuevos$lstat, bandas_est[,3],col='BLACK')
legend('topright', legend = c('Recta de regresión', 'Bandas de confianza al 90%'), lwd = 3, col = c('red', 'black'))
```

4. Realiza un diagnóstico de los residuos. Si falla alguna de las condiciones, busca una (o varias) posible solución.

```{r}
# Diagnóstico de linealidad y homocedasticidad
residuos <- residuals(lm_crim_lstat)
predichos <- fitted.values(lm_crim_lstat)
plot(predichos, residuos, col='BLUE', main = 'Gráfica de residuos')
abline(h=0, lty=2)

# Diagnóstico de normalidad de los residuos
qqnorm(residuos, col='BLUE')
qqline(residuos)
shapiro.test(residuos)
```

Siendo el resultado del p-valor prácticamente $0$ en el test de Shapiro_Wilk, rechazamos la hipótesis nula de normalidad de los residuos. Por tanto, hay evidencia estadística a favor de que los residuos no siguen una distribución normal.

En cuanto a las gráficas, la primera de ellas muestra la no linealidad de los residuos, ya que no aparecen como una nube aleatoria de puntos alrededor de la recta $y = 0$. Además, observamos que la varianza de los mismos aumenta a medida que incrementan los valores predichos, indicando un claro fallo de heterocedasticidad. La segunda gráfica confirma lo que deducido previamente del test de Shapiro-Wilk, es decir, que los residuos no siguen una distribución normal, debido a desviaciones importantes de la línea diagonal, especialmente en los extremos o colas. 

En conclusión, el modelo simple no es adecuado para estos datos debido a fallos en la linealidad, homocedasticidad y normalidad de los residuos. Una posible solución es aplicar una transformación logarítmica a la variable respuesta (`crim`) para estabilizar la varianza y mejorar el ajuste.

```{r}
# Aplicamos la transformación logarítmica
lm_crim_log <- lm(log(crim)~lstat, data = boston, na.action = na.exclude)
summary(lm_crim_log)

# Representamos el diagrama de dispersión junto con la recta de regresión y las bandas de estimación al 90%
min <- range(boston$lstat)[1]; max <- range(boston$lstat)[2]
nuevos <- data.frame(list(lstat = seq(min,max,length=100)))
bandas_est <- predict(lm_crim_log, newdata = nuevos, interval = "confidence", level = 0.90)

plot(boston$lstat, log(boston$crim), col='blue', main = 'Diagrama de dispersión transformación logarítmica', xlab = 'lstat', ylab = 'log(crim)', type = "p")
abline(coef = coef(lm_crim_log), col='RED')
lines(nuevos$lstat, bandas_est[,2],col='BLACK')
lines(nuevos$lstat, bandas_est[,3],col='BLACK')
legend('topright', legend = c('Recta de regresión', 'Bandas de confianza al 90%'), lwd = 3, col = c('red', 'black'))

# Diagnóstico de linealidad y homocedasticidad
residuos <- residuals(lm_crim_log)
predichos <- fitted.values(lm_crim_log)
plot(predichos, residuos, col='blue', main = 'Gráfica de residuos transformación logarítmica')
abline(h=0, lty=2)

# Diagnóstico de normalidad de los residuos
qqnorm(residuos, col='blue')
qqline(residuos)
shapiro.test(residuos)
```

Al realizar la transformación logarítmica, el coeficiente se entiende ahora como que por cada aumento de una unidad (un punto porcentual) en la población de nivel socioeconómico bajo (`lstat`), se espera que la tasa de criminalidad (`crim`) aumente aproximadamente en un `r round(lm_crim_log$coefficients[2], 4) * 100`%.

Observamos en el gráfico de dispersión que esta vez los puntos están distribuidos de forma mucho más homogénea alrededor de la recta. Además, en la gráfica de dispersión de los residuos detectamos la linealidad y homocedasticidad que buscábamos, al aparecer los residuos como una nube aleatoria de puntos alrededor de la recta horizontal y la varianza ser constante a lo largo del eje de valores predichos.

Para el test de normalidad de Shapiro-Wilk, obtenemos un p-valor de $0.02745$, por lo que, si consideráramos una significancia estadística del $1$% ($\alpha = 0.01$), podríamos rechazar la hipótesis nula y no tener suficiente evidencia estadística a favor de la no normalidad de los residuos.

Por lo que, pese a que el $R^2$ es ligeramente menor (seguimos explicando algo más del 40% de la varianza), se puede considerar que los fallos del anterior modelo se han solucionado con esta transformación.

<u>Ejercicio 2:</u> Considera la variable respuesta `crim` relacionándola con el predictor `medv`.

1. Evalúa el efecto de `medv` sobre `crim`.

```{r}
lm_crim_medv <- lm(crim ~ medv, data = boston)
summary(lm_crim_medv)
```

La variable predictora `medv`, que es muy significativa (al 0%), tiene un efecto negativo sobre `crim`: cuando `medv` aumenta una unidad, `crim` se reduce en `r -lm_crim_medv$coefficients[2]`.

2. Obtén la recta de mínimos cuadrados. Interpreta los resultados obtenidos (coeficientes, significatividad, $R^2$, contraste del modelo, etc...).

Obtenemos la siguiente recta de mínimos cuadrados: $$\hat{\text{crim}} = -8.5183 - 0.255 \, \, \text{medv},$$

siendo $\beta_0 =$ `r lm_crim_medv$coefficients[1]` el intercepto y $\beta_1 =$ `r lm_crim_medv$coefficients[2]` la pendiente de la recta.

Como se ha comentado en el apartado anterior, aparentemente `medv` es muy significativa y si aumenta una unidad, produce una disminución en `crim` de `r -lm_crim_medv$coefficients[2]`. El p-valor del estadístico F es prácticamente $0$, por lo que se rechaza la hipótesis nula de que ninguna variable es significativa. Sin embargo, el $R^2$ es relativamente bajo ($0.2736$) por lo que la varianza del modelo no está siendo explicada (estamos dejando casi el 75% de la vaarianza del modelo sin explicar).

3. Dibuja el diagrama de dispersión, la recta de regresión y las bandas de predicción al 90%.

```{r}
# Obtención de las bandas de estimación
min2 <- range(boston$medv)[1]; max2 <- range(boston$medv)[2]
nuevos2 <- data.frame(list(medv = seq(min2,max2,length=100)))
bandas_est2 <- predict(lm_crim_medv, newdata = nuevos2, interval = "prediction", level = 0.90)

# Representación gráfica
plot(boston$medv, boston$crim, col='BLUE', main = 'Diagrama de dispersión', xlab = 'medv', ylab = 'crim')
abline(coef=coef(lm_crim_medv), col='RED')
lines(nuevos2$medv, bandas_est2[,2],col='BLACK')
lines(nuevos2$medv, bandas_est2[,3],col='BLACK')
legend('topright', legend = c('Recta de regresión', 'Bandas de predicción al 90%'), lwd = 3, col = c('red', 'black'))
```

4. Realiza un análisis de los residuos.

```{r}
# Diagnóstico de linealidad y homocedasticidad
residuos2 <- residuals(lm_crim_medv)
predichos2 <- fitted.values(lm_crim_medv)
plot(predichos2, residuos2, col='BLUE', main = 'Gráfica de residuos', xlab = 'predichos', ylab = 'residuos')
abline(h=0,lty=2)

# Diagnóstico de normalidad de los residuos
qqnorm(residuos2, col='BLUE')
qqline(residuos2)
shapiro.test(residuos2)
```

Observando el resultado del p-valor cercano a $0$ en el test de normalidad de Shapiro-Wilk, rechazamos la hipótesis nula de normalidad de los residuos, es decir, hay evidencia estadística para afirmar que los residuos no siguen una distribución normal.

En cuanto a las gráficas, la primera muestra que la relación no es puramente lineal ya que la dispersión de los residuos se amplía a medida que los valores predichos aumentan y forman una especie de U invertida, representando un fallo de heterocedasticidad. La segunda gráfica, confirma que los residuos no siguen una distribución normal, debido a desviaciones importantes de la línea diagonal, especialmente en los extremos. 

En cuanto a las gráficas, la primera de ellas muestra la no linealidad de los residuos, ya que no aparecen como una nube aleatoria de puntos alrededor de la recta $y = 0$. Además, observamos que la dispersión de los mismos aumenta a medida que incrementan los valores predichos, formando una especie de U, indicando un claro fallo de heterocedasticidad. La segunda gráfica confirma lo que deducido previamente del test de Shapiro-Wilk, es decir, que los residuos no siguen una distribución normal, debido a desviaciones importantes de la línea diagonal, especialmente en el extremo o cola superior. 

5. ¿Te parece adecuado haber realizado regresión lineal o es preferible otro tipo de regresión?. Ajusta el modelo que te parezca más adecuado.

El modelo simple no es adecuado para estos datos debido a fallos en la linealidad, homocedasticidad y normalidad de los residuos. Una posible solución, viendo la forma de la dispersión de los residuos, es realizar un ajuste parabólico sobre la variable predictora `medv` para estabilizar la varianza y mejorar el ajuste.

En primer lugar, realizamos una transformación logarítmica de los datos.

```{r}
lm_medv_lineal <- lm(log(crim) ~ medv, data = boston)
summary(lm_medv_lineal)

# Planteamos una solución con un ajuste parabólico sobre la variable predictora medv
lm_medv_parab <- lm(log(crim) ~ medv + I(medv^2), data = boston)
summary(lm_medv_parab)

# Representación del ajuste
plot(boston$medv, log(boston$crim), col = 'black', main = "Diagrama de dispersión", xlab = "medv", ylab = "log(crim)")
abline(coef = coef(lm_medv_lineal), lwd = 2, col = 'red')
lines(sort(boston$medv), fitted(lm_medv_parab)[order(boston$medv)], col = 'darkcyan', lwd = 3)
legend('topright', legend = c('Modelo lineal', 'Modelo parabólico'), lwd = 3, col = c('red', 'darkcyan'))
```

Para saber si el coeficiente cuadrático es significativo, utilizamos la función `anova`. Además, para comparar el nuevo modelo parabólico con el modelo lineal, calculamos el coeficiente de información de Akaike con la función `AIC`. 

```{r}
anova(lm_medv_parab, lm_medv_lineal)
print(paste("AIC del modelo lineal:", AIC(lm_medv_lineal)))
print(paste("AIC del modelo parabólico:", AIC(lm_medv_parab)))
```

Obtenemos un p-valor significativo al 0%, por lo que rechazamos la hipótesis nula y concluimos que hay evidencia estadística a favor de la significatividad del coeficiente cuadrático. Comprobamos también que el valor del AIC se reduce, con lo cual hemos mejorado con nuestro nuevo modelo.

Si hacemos lo mismo para el caso cúbico, empleando la función `poly`, y comparamos con el caso parabólico (para comparar la introducción de polinomios ortogonales) tenemos

```{r}
lm_medv_3 <- lm(log(crim) ~ poly(medv, 3), data = boston)
anova(lm_medv_parab, lm_medv_3, test = 'F')
print(paste("AIC del modelo parabólico:", AIC(lm_medv_3)))
```

El p-valor que obtenemos ya no es menor que el nivel de significación $\alpha = 0.05$, por lo que no rechazamos la hipótesis nula y hay evidencia estadística para afirmar que el coeficiente de orden 3 no es significativo. Además, obtenemos un AIC superior. Por tanto, nos quedamos con el modelo parabólico.

Observamos que con nuestro nuevo modelo obtenemos un valor $R^2$ ajustado de $0.4951$, es decir, explicamos aproximadamente la mitad de la varianza de la variable `crim`, frente a menos del $30%$ que explicábamos con el modelo de regresión lineal original.

Realicemos un análisis de residuos.

```{r}
# Diagnóstico de linealidad y homocedasticidad
  # Test para la homocedasticidad
bptest(lm_medv_parab)

  # Representación gráfica
residuos_trans <- residuals(lm_medv_parab)
predichos_trans <- fitted.values(lm_medv_parab)

plot(predichos_trans, residuos_trans, col='blue', main = 'Gráfica de residuos con ajuste parabólico', xlab = 'predichos', ylab = 'residuos')
abline(h = 0, lty = 2)

# Diagnóstico de normalidad de los residuos
qqnorm(residuos_trans, col='blue')
qqline(residuos_trans)
shapiro.test(residuos_trans)
```

Observamos en la gráfica de dispersión de los residuos que la linealidad de los mismos ha mejorado. Además, obtenemos para el test de Breusch-Pagan un p-valor de $0.3329$, por lo que no rechazamos la hipótesis nula y concluimos que no hay suficiente evidencia estadística para rechazar la homocedasticidad de los residuos.

Por lo que respecta al test de normalidad de Shapiro-Wilk, obtenemos un p-valor de $0.05507$, es decir, tampoco rechazamos la hipótesis nula. En consecuencia, no existe evidencia estadística suficiente para afirmar que los residuos no siguen una distribución normal.

En resumen, nuestro nuevo modelo parabólico ha resuelto los fallos en la linealidad, homocedasticidad y normalidad que presentaban los residuos del modelo lineal. Además, hemos conseguido aumentar el porcentaje de varianza de la variable `crim` explicada por el modelo.

6. ¿Qué tasa de criminalidad se espera para aquellos barrios con un precio mediano de la vivienda de 30000 dólares? ¿Y 10000? ¿Y 100000? Calcula e interpreta los intervalos de confianza y de predicción.

Realizaremos las predicciones y los cálculos de los intervalos de confianza y predicción con el modelo parabólico del apartado anterior, el cual habíamos entrenado sobre los datos transformados con una transformación logarítmica. Deberemos, por tanto, realizar la transformación inversa (función exponecial) a los resultados antes de darles una interpretación.

```{r}
medv_pred_log <- predict(lm_medv_parab, newdata = data.frame(medv = c(30, 10, 100)), interval = "prediction", level = 0.90)
int_pred <- exp(medv_pred_log)

medv_conf_log <- predict(lm_medv_parab, newdata = data.frame(medv = c(30, 10, 100)), interval = "confidence", level = 0.90)
int_conf <- exp(medv_conf_log)
```

Para aquellos barrios con un precio mediano de la vivienda de 30000 dólares la tasa de criminalidad esperada es de `r round(int_conf[1,1], 4)`, con un intervalo de confianza de [`r round(int_conf[1,2], 4)`, `r round(int_conf[1,3], 4)`] y un intervalo de predicción de [`r round(int_pred[1,2], 4)`, `r round(int_pred[1,3], 4)`].

Por lo que respecta a los barrios con precio mediano de la vivienda de 10000 dólares, la tasa de criminalidad esperada es de `r round(int_conf[2,1], 4)`, con un intervalo de confianza de [`r round(int_conf[2,2], 4)`, `r round(int_conf[2,3], 4)`] y un intervalo de predicción de [`r round(int_pred[2,2], 4)`, `r round(int_pred[2,3], 4)`].

Por último, para los barrios con precio mediano de la vivienda de 100000 dólares, la tasa de criminalidad esperada es de `r int_conf[3,1]`, con un intervalo de confianza de [`r int_conf[3,2]`, `r int_conf[3,3]`] y un intervalo de predicción de [`r int_pred[3,2]`, `r int_pred[3,3]`].

Notamos que obtenemos valores desproporcionados para el caso de los barrios con precio mediano de la vivienda de 100000 dólares. Esto se debe a que nuestro modelo ha sido entrenado para valores de la variable `medv` entre $7.2$ y $50.0$, por lo que tratar de hacer predicciones para valores tan alejados de este intervalo puede llevar a resultados fuera del rango lógico de la variable `crim`.

<u>Ejercicio 3:</u>

1. Encuentra el número óptimo de variables a incluir en un modelo predictivo de `crim`, según los criterios $R^2$, BIC y CP, utilizando la metodología RegSubsets. Indica brevemente en que consiste esta metodología.

```{r}
reg_crim <- regsubsets(crim ~ ., data = boston)
summary <- summary(reg_crim)
```

```{r}
lm1 <- lm(crim ~ lstat, data = boston)
lm2 <- lm(crim ~ ptratio + lstat, data = boston)
lm3 <- lm(crim ~ ptratio + lstat + nox, data = boston)
lm4 <- lm(crim ~ ptratio + lstat + rm + indus, data = boston)
lm5 <- lm(crim ~ ptratio + lstat + rm + indus + chas, data = boston)
lm6 <- lm(crim ~ ptratio + lstat + rm + indus + chas + medv, data = boston)
lm7 <- lm(crim ~ ptratio + lstat + rm + indus + chas + medv + dis, data = boston)
lm8 <- lm(crim ~ ptratio + lstat + rm + indus + chas + medv + dis + age, data = boston)

summary(lm1)
summary(lm2)
summary(lm3)
summary(lm4)
summary(lm5)
summary(lm6)
summary(lm7)
summary(lm8)
```

```{r}
resultado <- cbind(summary$rsq, summary$adjr2, summary$cp, summary$bic)
colnames(resultado) <- c('Rsq', 'RsqAdj', 'Cp', 'BIC')

length(summary$adjr2)

par(mfrow = c(1,3))
plot(1:8, summary$adjr2, xlab = "Variables", main = "Coef. Det. Ajustado",
     type="b")
abline(v = which.max(summary$adjr2), col = 2)
plot(1:8, summary$cp, xlab = "Variables", main = "Cp de Mallows",
     type='b')
abline(v = which.min(summary$cp), col = 2)
plot(1:8, summary$bic, xlab = "Variables", main = "BIC",
     type = "b")
abline(v = which.min(summary$bic), col = 2)
par(mfrow = c(1,1))

AIC(lm1, lm2, lm3, lm4, lm5, lm6, lm7, lm8)
```

Observamos que el modelo 5 tiene el valor de BIC más bajo (-142.41) y el modelo 8 el AIC más bajo (990.59), por lo que significa que son los modelos que mejor equilibran el ajuste. El modelo 8 contiene el $R^2$ ajustado más elevado pese a tener `age` como variable no significativa, por lo que es el que mejor explica la varianza de la variable respuesta. 
Por último, el modelo 8 tiene el CP de Mallows más bajo por lo que tiene menor error de predicción. 

Teniendo en cuenta estos resultados, podríamos elegir como mejor modelo el 5, por penalización de complejidad (menos variables) o el modelo 8 si preferimos una predicción más exacta ya que el BIC suele ser mejor en modelos más simples (como en el 5).

  - ¿Qué variables incluye el modelo obtenido? (Seleccionar el criterio que más os guste). Interpreta los coeficientes obtenidos. ¿Tienen todas sentido?. ¿Son significativos?.
  
El modelo final incluye 8 variables: `ptratio`, `lstat`, `rm`, `indus`, `chas`, `medv`, `dis` y `age`.

La mayoria de los coeficientes tienen la dirección esperada. Las características de desventaja (`ptratio`, `lstat`, `indus`) aumentan `crim` (signo positivo), mientras que las características de ventaja (`chas`, `medv`, `dis`) lo disminuyen (signo negativo). El coeficiente de `rm` (número promedio de habitaciones) es positivo. Esto va en contra de la expectativa de que más habitaciones (casas mas grandes/caras) indicarian mayor riqueza y, por lo tanto, menor criminalidad. Esto sugiere que `rm` está correlacionada con alguna otra variable.

2. Selecciona el mejor modelo con el método stepwise. Indica brevemente en que consiste esta metodología y contesta a las siguientes preguntas:

  - ¿Qué modelo piensas que es mejor? (Entre este y el/los obtenido/s mediante Regsubsets).
  
```{r}
crim_global <- lm(crim ~., data = boston)
step_crim <- step(crim_global, direction = 'both', trace = 0) 
summary(step_crim)
```

   Coincide con una de las opciones planteadas con la otra metodología (modelo 8), de acuerdo con los estadísticos contrastados, por lo que este modelo sería la mejor opción.

  - ¿Qué % de la varianza de `crim` explica el modelo?
  
    El modelo con stepwise se compone de 8 variables y explica un 60% de la variación en la tasa de criminalidad (`crim`). La mayoria de los predictores son muy significativos, exceptuando la variable `age`.

  - ¿Cuál es el efecto de la variable `chas` sobre `crim`?
  
    El coeficiente de `chas` -2.23558 significa que, manteniendo todas las demás variables constantes, las areas que colindan con el río Charles (`chas` = 1) tienen una tasa de criminalidad 2,24 unidades menor que las areas que no coinciden con el río.

3. Con el modelo obtenido con stepwise, realiza el diagnóstico de tu modelo, sin emprender ninguna acción, e indica los problemas que presenta.

Repetimos que las variables explican un 60% de la varianza del modelo y que este en su conjunto es altamente significativo (estadístico F con un p-valor cercano a 0). Esto indica que al menos una de las variables predictoras es útil para el modelo.

Las variables `indus`, `chas`, `rm`, `ptratio`, y `lstat` son las que tienen el impacto más significativo en la predicción de la tasa de criminalidad. La variable age no es estadísticamente significativa, lo que hace que su inclusión no mejora significativamente la predicción del modelo.

Se podría considerar hacer un modelo final eliminando `age` y `rm` por su supuesta multicolinealidad mencionada anteriormente para ver si esto mejora el $R^2$ ajustado. 

4. Emprende ahora las acciones que te parezcan oportunas e indica los problemas que has conseguido solucionar o mejorar un poco.

```{r}
lm_crim_nuevo <- lm(crim ~ lstat + indus + chas + dis + ptratio + medv, data=boston)
summary(lm_crim_nuevo)
```

Eliminando las dos variables mencionadas anteriormente, vemos que ahora medv deja de ser significativa. La eliminamos y volvemos a hacer la regresión.

```{r}
lm_crim_nuevo <- lm(crim ~ lstat + indus + chas + dis + ptratio, data=boston)
summary(lm_crim_nuevo)
```

Ahora vemos que todas las variables son significativas y sus coeficientes tienen sentido. Además el $R^2$ es mayor de 50% y el estadistico F confirma que al menos una de las variables es significativa.

5. Obtén la predicción de la tasa de criminalidad para un barrio en la mediana de los predictores en el modelo escogido. _Notar que las variables categóricas se tratan de diferente manera, no hay mediana_.

```{r}
med_ptratio <- median(boston$ptratio)
med_lstat <- median(boston$lstat)
med_indus <- median(boston$indus)
med_dis <- median(boston$dis)

datos_medianos <- data.frame(
  ptratio = med_ptratio,
  lstat = med_lstat,
  indus = med_indus,
  chas = 0, #asumimos chas = 0
  dis = med_dis
)

print(prediccion_crim <- predict(lm_crim_nuevo, newdata = datos_medianos))
```

La tasa de incidentes criminales por $10,000$ habitantes en este barrio hipotético (con características medianas) es de $1.63$
